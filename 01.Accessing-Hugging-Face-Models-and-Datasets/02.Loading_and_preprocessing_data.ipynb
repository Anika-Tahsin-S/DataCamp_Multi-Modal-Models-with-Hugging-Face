{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "text = \"Do you need more eclairs?\"\n",
    "\n",
    "print(tokenizer.backend_tokenize.normalizer.normalize_str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(text, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "checkpoint = \"Salesforce/blip-image-captioning-base\"\n",
    "model = BlipForConditionalGeneration.from_pretrained(checkpoint)\n",
    "feature_extractor = BlipProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_dataset(\"nlphuji/flickr30k\")['test'][11][\"image\"]\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "dataset = load_dataset(\"CSTR-Edinburgh/vctk\")[\"train\"]\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AudioProcessor\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "audio_pp = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Text tokenizing\n",
    "- Load the first caption from the image at index 5 of the dataset.\n",
    "- Load the tokenizer of the pretrained model: distilbert/distilbert-base-uncased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first caption from the image at index 5\n",
    "text = dataset[5][\"caption\"][0]\n",
    "\n",
    "# Load the tokenizer of the pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Image preprocessing\n",
    "- Load the image from the element at index 5 of the dataset.\n",
    "- Load the image processor (BlipProcessor) of the pretrained model: Salesforce/blip-image-captioning-base.\n",
    "- Execute the processor on image, making sure to specify that PyTorch tensors (pt) are required.\n",
    "- Use the .generate() method to create a caption using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image from index 5 of the dataset\n",
    "image = dataset[5][\"image\"]\n",
    "\n",
    "# Load the image processor of the pretrained model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Preprocess the image\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate a caption using the model\n",
    "output = model.generate(**inputs)\n",
    "print(f'Generated caption: {processor.decode(output[0])}')\n",
    "print(f'Original caption: {dataset[5][\"caption\"][0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Audio preprocessing\n",
    "- Resample the audio to a frequency of 16,000 Hz in the dataset using the .cast_column() method.\n",
    "- Load the audio processor using the pretrained openai/whisper-small model.\n",
    "- Preprocess the audio data of the first datapoint using the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the audio to a frequency of 16,000 Hz\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# Load the audio processor\n",
    "processor = AutoProcessor.from_pretrained('openai/whisper-small')\n",
    "\n",
    "# Preprocess the audio data of the 0th dataset element\n",
    "audio_pp = processor(dataset[0][\"audio\"][\"array\"], sampling_rate=16_000, padding=True, return_tensors=\"pt\")\n",
    "make_spectrogram(audio_pp[\"input_features\"][0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
